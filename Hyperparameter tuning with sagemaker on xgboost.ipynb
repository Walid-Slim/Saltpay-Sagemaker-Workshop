{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a machine learning model on sagemaker\n",
    "\n",
    "The objective of this workshop is to understand the steps needed to deploy a machine learning model on SageMaker.\n",
    "To do that, it is necessary to understand how SageMaker works, and what it needs to work properly.\n",
    "\n",
    "For that, we will understand how to preprocess the data, and how to properly parameter the SageMaker sdk functions.\n",
    "\n",
    "Objectives :\n",
    "- Understand the problem\n",
    "- Understand how SageMaker works\n",
    "- Preprocess the data in a manner that works with SageMaker\n",
    "- Understand how to use hyperparameter optimisation with SageMaker to get the best model\n",
    "- Understand the steps to deploy a model with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "#bucket = \"s3://sagemaker-data-workshop\"\n",
    "bucket_name=boto3.client(\"s3\").list_buckets()[\"Buckets\"][0][\"Name\"]\n",
    "bucket=\"s3://\"+bucket_name\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "\n",
    "from sagemaker.predictor import Predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Plan of actions\n",
    "![Plan of actions](Workflow-diagram.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important notions to keep in mind :\n",
    "- The model will only accept data without headers, so we use numpy instead of pandas\n",
    "- The target of the model needs to be the first column (sagemaker default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available from UCI Machine Learning. The aim for this task is to determine age of an Abalone (a kind of shellfish) from its physical measurements. At the core, it's a regression problem. The dataset contains several features - sex (categorical), length (continuous), diameter (continuous), height (continuous), whole_weight (continuous), shucked_weight (continuous), viscera_weight (continuous), shell_weight (continuous) and rings (integer).Our goal is to predict the variable rings which is a good approximation for age (age is rings + 1.5).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Since we get a headerless CSV file we specify the column names here.\n",
    "feature_columns_names = [\n",
    "    'sex', # M, F, and I (infant)\n",
    "    'length', # Longest shell measurement\n",
    "    'diameter', # perpendicular to length\n",
    "    'height', # with meat in shell\n",
    "    'whole_weight', # whole abalone\n",
    "    'shucked_weight', # weight of meat\n",
    "    'viscera_weight', # gut weight (after bleeding)\n",
    "    'shell_weight'] # after being dried\n",
    "\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64,\n",
    "}\n",
    "\n",
    "label_column_dtype = {'rings': \"float64\"} # +1.5 gives the age in years\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        bucket+\"/abalone.csv\",\n",
    "        header=None,\n",
    "        names=feature_columns_names + [label_column],\n",
    "        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),\n",
    "        )\n",
    "    \n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "                                  )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "                                      )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "                                  )\n",
    "    \n",
    "    train, validation, test = np.split(df, [int(0.7 * len(df)), int(0.85 * len(df))])\n",
    "    \n",
    "    y = train.pop(\"rings\")\n",
    "    X_pre = preprocess.fit_transform(train)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    train = np.concatenate((y_pre, X_pre), axis=1)\n",
    "    \n",
    "    y = validation.pop(\"rings\")\n",
    "    X_pre = preprocess.transform(validation)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    validation = np.concatenate((y_pre, X_pre), axis=1)\n",
    "    \n",
    "\n",
    "    y = test.pop(\"rings\")\n",
    "    X_pre = preprocess.transform(test)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    test = np.concatenate((y_pre, X_pre), axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    pd.DataFrame(train).to_csv(\"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        \"validation.csv\", header=False, index=False\n",
    "                                   )\n",
    "    pd.DataFrame(test).to_csv(\"test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource(\"s3\").Bucket(bucket_name).Object( #Upload train data to s3\n",
    "    (\"train/train.csv\")\n",
    ").upload_file(\"train.csv\")\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket_name).Object( #Upload validation data to s3\n",
    "    (\"validation/validation.csv\")\n",
    ").upload_file(\"validation.csv\")\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket_name).Object( #Upload test data to s3\n",
    "    (\"test/test.csv\")\n",
    ").upload_file(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each step in SageMaker has its own input : Traininginput for training, transforminput for batch_transform,\n",
    "# ProcessingInput for processing, etc...\n",
    "\n",
    "s3_input_train = TrainingInput( \n",
    "    s3_data=\"{}/train/train.csv\".format(bucket), content_type=\"csv\"\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    s3_data=\"{}/validation/validation.csv\".format(bucket), content_type=\"csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the image that runs inside the EC2 machine that trains the model\n",
    "- Specify the parameters of the models, and the parameters that we are going to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"xgboost\",\n",
    "                                          boto3.Session().region_name,\n",
    "                                          \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=\"{}/output\".format(bucket),\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    eval_metric=\"rmse\",\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50, #Number of trees (num_estimator)\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"alpha\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"), #Parameter for L1 regularization\n",
    "    \"lambda\": ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"), #Parameter for L2 regularization\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of [EC2 Machines](https://aws.amazon.com/ec2/instance-types/?trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_IBERIA&sc_publisher=Google&sc_category=Cloud%20Computing&sc_country=IBERIA&sc_geo=EMEA&sc_outcome=acq&sc_detail=amazon%20ec2%20instance&sc_content=%7Bad%20group%7D&sc_matchtype=e&sc_segment=536392708261&sc_medium=ACQ-P%7CPS-GO%7CBrand%7CDesktop%7CSU%7CCloud%20Computing%7CEC2%7CIBERIA%7CEN%7CSitelink&s_kwcid=AL!4422!3!536392708261!e!!g!!amazon%20ec2%20instance&ef_id=EAIaIQobChMI38OXroWC9gIVSgKLCh3pDwytEAAYASABEgJuvfD_BwE:G:s&s_kwcid=AL!4422!3!536392708261!e!!g!!amazon%20ec2%20instance) , their specs and their characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................!\n"
     ]
    }
   ],
   "source": [
    "objective_metric_name = \"validation:rmse\"\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb,                     \n",
    "    objective_metric_name,   \n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    "    strategy=\"Random\",         #Random hyperparameter search. Other alternative is Bayesian\n",
    "                               #SageMaker doesn't support gridsearch\n",
    "    objective_type=\"Minimize\", #We want to minimize RMSE. If it was precision, we would have \"Maximize\"\n",
    "    base_tuning_job_name=\"T1\"  #Name for the tuning job\n",
    ")\n",
    "\n",
    "tuner.fit({\"train\": s3_input_train, \"validation\": s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name=tuner.latest_tuning_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 training jobs have completed\n"
     ]
    }
   ],
   "source": [
    "tuning_job_result = sage_client.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "\n",
    "job_count = tuning_job_result[\"TrainingJobStatusCounters\"][\"Completed\"]\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "\n",
    "is_minimize = objective[\"Type\"] != \"Maximize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training jobs with valid objective: 3\n",
      "{'lowest': 2.19281005859375, 'highest': 2.343100070953369}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>lambda</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.524426</td>\n",
       "      <td>0.057152</td>\n",
       "      <td>4.0</td>\n",
       "      <td>T1-220216-1724-002-130f1969</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2.19281</td>\n",
       "      <td>2022-02-16 17:26:33+00:00</td>\n",
       "      <td>2022-02-16 17:27:21+00:00</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.027243</td>\n",
       "      <td>4.621644</td>\n",
       "      <td>9.0</td>\n",
       "      <td>T1-220216-1724-003-55e12879</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2.32833</td>\n",
       "      <td>2022-02-16 17:26:28+00:00</td>\n",
       "      <td>2022-02-16 17:27:11+00:00</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126621</td>\n",
       "      <td>0.171751</td>\n",
       "      <td>9.0</td>\n",
       "      <td>T1-220216-1724-001-b6aff576</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2.34310</td>\n",
       "      <td>2022-02-16 17:26:22+00:00</td>\n",
       "      <td>2022-02-16 17:27:03+00:00</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha    lambda  max_depth              TrainingJobName  \\\n",
       "1  3.524426  0.057152        4.0  T1-220216-1724-002-130f1969   \n",
       "0  1.027243  4.621644        9.0  T1-220216-1724-003-55e12879   \n",
       "2  0.126621  0.171751        9.0  T1-220216-1724-001-b6aff576   \n",
       "\n",
       "  TrainingJobStatus  FinalObjectiveValue         TrainingStartTime  \\\n",
       "1         Completed              2.19281 2022-02-16 17:26:33+00:00   \n",
       "0         Completed              2.32833 2022-02-16 17:26:28+00:00   \n",
       "2         Completed              2.34310 2022-02-16 17:26:22+00:00   \n",
       "\n",
       "            TrainingEndTime  TrainingElapsedTimeSeconds  \n",
       "1 2022-02-16 17:27:21+00:00                        48.0  \n",
       "0 2022-02-16 17:27:11+00:00                        43.0  \n",
       "2 2022-02-16 17:27:03+00:00                        41.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tuner = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "\n",
    "full_df = tuner.dataframe()\n",
    "\n",
    "if len(full_df) > 0:\n",
    "    df = full_df[full_df[\"FinalObjectiveValue\"] > -float(\"inf\")]\n",
    "    if len(df) > 0:\n",
    "        df = df.sort_values(\"FinalObjectiveValue\", ascending=is_minimize)\n",
    "        print(\"Number of training jobs with valid objective: %d\" % len(df))\n",
    "        print({\"lowest\": min(df[\"FinalObjectiveValue\"]), \"highest\": max(df[\"FinalObjectiveValue\"])})\n",
    "        pd.set_option(\"display.max_colwidth\", None)  # Don't truncate TrainingJobName\n",
    "    else:\n",
    "        print(\"No training jobs have reported valid results yet.\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model found so far:\n",
      "{'TrainingJobName': 'T1-220216-1724-002-130f1969', 'TrainingJobArn': 'arn:aws:sagemaker:eu-west-1:775923162736:training-job/t1-220216-1724-002-130f1969', 'CreationTime': datetime.datetime(2022, 2, 16, 17, 24, 20, tzinfo=tzlocal()), 'TrainingStartTime': datetime.datetime(2022, 2, 16, 17, 26, 33, tzinfo=tzlocal()), 'TrainingEndTime': datetime.datetime(2022, 2, 16, 17, 27, 21, tzinfo=tzlocal()), 'TrainingJobStatus': 'Completed', 'TunedHyperParameters': {'alpha': '3.524426056570139', 'lambda': '0.05715202020575164', 'max_depth': '4'}, 'FinalHyperParameterTuningJobObjectiveMetric': {'MetricName': 'validation:rmse', 'Value': 2.19281005859375}, 'ObjectiveStatus': 'Succeeded'}\n"
     ]
    }
   ],
   "source": [
    "if tuning_job_result.get(\"BestTrainingJob\", None):\n",
    "    print(\"Best model found so far:\")\n",
    "    print(tuning_job_result[\"BestTrainingJob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': '3.524426056570139',\n",
       " 'lambda': '0.05715202020575164',\n",
       " 'max_depth': '4'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_job_result[\"BestTrainingJob\"][\"TunedHyperParameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params=tuning_job_result[\"BestTrainingJob\"][\"TunedHyperParameters\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an xgboost model with best parameters and deploying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=\"{}/output\".format(bucket),\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    eval_metric=\"rmse\",\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    \n",
    "    #Getting the best params\n",
    "    alpha=best_params[\"alpha\"],\n",
    "    reg_lambda=best_params[\"lambda\"],\n",
    "    max_depth=best_params[\"max_depth\"]   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-16 17:29:19 Starting - Starting the training job...\n",
      "2022-02-16 17:29:21 Starting - Launching requested ML instancesProfilerReport-1645032559: InProgress\n",
      "......\n",
      "2022-02-16 17:30:48 Starting - Preparing the instances for training......\n",
      "2022-02-16 17:31:44 Downloading - Downloading input data...\n",
      "2022-02-16 17:32:17 Training - Training image download completed. Training in progress.\n",
      "2022-02-16 17:32:17 Uploading - Uploading generated training model\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2022-02-16:17:32:12:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2022-02-16:17:32:12:INFO] Path /opt/ml/input/data/validation does not exist!\u001b[0m\n",
      "\u001b[34m[2022-02-16:17:32:12:INFO] File size need to be processed in the node: 0.43mb. Available memory size in the node: 8009.94mb\u001b[0m\n",
      "\u001b[34m[2022-02-16:17:32:12:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[17:32:12] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[17:32:12] 2923x10 matrix with 29230 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:7.22523\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:5.32614\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:4.06698\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:3.25465\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:2.75487\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:2.4497\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:2.28139\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:2.17737\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:2.11607\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:2.07132\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:2.04671\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:2.01677\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:1.9872\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:1.9615\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:1.94451\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:1.92819\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:1.91736\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:1.90838\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:1.89191\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:1.88266\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:1.86519\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:1.86059\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:1.84191\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:1.82171\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:1.81713\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:1.81306\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:1.79891\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:1.78405\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:1.78097\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:1.76854\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:1.75861\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:1.74682\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:1.74215\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:1.7313\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:1.72145\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:1.70746\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:1.70535\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:1.69521\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:1.69008\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:1.68142\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:1.67129\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:1.66114\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:1.65495\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:1.64001\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:1.63139\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:1.62325\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:1.61823\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:1.60903\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:1.60507\u001b[0m\n",
      "\u001b[34m[17:32:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:1.59576\u001b[0m\n",
      "\n",
      "2022-02-16 17:32:31 Completed - Training job completed\n",
      "Training seconds: 41\n",
      "Billable seconds: 41\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({\"train\": s3_input_train}) # Put the whole data instead of just train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we are going to host an endpoint, we can make real-time predictions from our model very easily, simply by making a http POST request. But first, we'll need to set up serializers and deserializers for passing our test_data NumPy arrays to the model behind the endpoint.\n",
    "\n",
    "Now, we'll use a simple function to:\n",
    "\n",
    "- Loop over our test dataset <br>\n",
    "- Split it into mini-batches of rows <br> \n",
    "- Convert those mini-batchs to CSV string payloads <br>\n",
    "- Retrieve mini-batch predictions by invoking the XGBoost endpoint <br>\n",
    "- Collect predictions and convert from the CSV output our model provides into a NumPy array <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-data-workshop/output/xgboost-2022-02-16-17-29-19-095/output/model.tar.gz'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_description=xgb.jobs[0].describe()\n",
    "model_path=os.path.join(model_description[\"OutputDataConfig\"][\"S3OutputPath\"],\n",
    "                         model_description[\"TrainingJobName\"],\n",
    "                         \"output\",\n",
    "                         \"model.tar.gz\"\n",
    "                       )\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model=sagemaker.Model(image_uri=container,model_data=model_path,role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "xgb_model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge', endpoint_name=\"xgb-endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialization is the process of converting a data object—a combination of code and data represented within a region of data storage—into a series of bytes that saves the state of the object in an easily transmittable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(f\"s3://{bucket_name}/test/test.csv\")\n",
    "predictor=Predictor(endpoint_name=\"xgb-endpoint\",\n",
    "                    sagemaker_session=sess,\n",
    "                    serializer=CSVSerializer()  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1)) #Mini batch\n",
    "    predictions = \"\"\n",
    "    for array in split_array:\n",
    "        predictions = \",\".join([predictions, predictor.predict(array).decode(\"utf-8\")]) #Make predictions and \n",
    "                                                                                        #Desereliaze data\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=\",\")\n",
    "\n",
    "\n",
    "predictions = predict(test.to_numpy()[:, 1:]) #remove the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(626, 11)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use batch transform when you need to do the following:\n",
    "\n",
    "- Preprocess datasets to remove noise or bias that interferes with training or inference from your dataset.\n",
    "- Get inferences from large datasets.\n",
    "- Run inference when you don't need a persistent endpoint.\n",
    "- Associate input records with inferences to assist the interpretation of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=xgb_model.transformer(instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    strategy=\"MultiRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    "    output_path=f\"s3://{bucket_name}/predictions\",\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy\n",
    "Specifies the number of records to include in a mini-batch for an HTTP inference request. A record is a single unit of input data that inference can be made on. For example, a single line in a CSV file is a record.\n",
    "\n",
    "To enable the batch strategy, you must set the SplitType property to Line, RecordIO, or TFRecord.\n",
    "\n",
    "To use only one record when making an HTTP invocation request to a container, set BatchStrategy to SingleRecord and SplitType to Line.\n",
    "\n",
    "To fit as many records in a mini-batch as can fit within the MaxPayloadInMB limit, set BatchStrategy to MultiRecord and SplitType to Line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[35mArguments: serve\u001b[0m\n",
      "\u001b[34m[2022-02-16 18:15:01 +0000] [1] [INFO] Starting gunicorn 19.9.0\u001b[0m\n",
      "\u001b[34m[2022-02-16 18:15:01 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2022-02-16 18:15:01 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2022-02-16 18:15:01 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
      "\u001b[34m[2022-02-16 18:15:01 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[34m[2022-02-16:18:15:01:INFO] Model loaded successfully for worker : 21\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[34m[2022-02-16:18:15:01:INFO] Model loaded successfully for worker : 22\u001b[0m\n",
      "\u001b[34m[2022-02-16 18:15:01 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[34m[2022-02-16 18:15:01 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[34m[2022-02-16:18:15:01:INFO] Model loaded successfully for worker : 23\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[34m[2022-02-16:18:15:01:INFO] Model loaded successfully for worker : 24\u001b[0m\n",
      "\u001b[35m[2022-02-16 18:15:01 +0000] [1] [INFO] Starting gunicorn 19.9.0\u001b[0m\n",
      "\u001b[35m[2022-02-16 18:15:01 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2022-02-16 18:15:01 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2022-02-16 18:15:01 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
      "\u001b[35m[2022-02-16 18:15:01 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[35m[2022-02-16:18:15:01:INFO] Model loaded successfully for worker : 21\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[35m[2022-02-16:18:15:01:INFO] Model loaded successfully for worker : 22\u001b[0m\n",
      "\u001b[35m[2022-02-16 18:15:01 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[35m[2022-02-16 18:15:01 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[35m[2022-02-16:18:15:01:INFO] Model loaded successfully for worker : 23\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[35m[2022-02-16:18:15:01:INFO] Model loaded successfully for worker : 24\u001b[0m\n",
      "\u001b[34m[2022-02-16:18:15:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2022-02-16:18:15:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2022-02-16:18:15:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2022-02-16:18:15:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2022-02-16T18:15:04.945:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.transform(f\"s3://{bucket_name}/test/test.csv\",\n",
    "                      content_type=\"text/csv\",\n",
    "                      join_source=\"Input\",\n",
    "                      split_type=\"Line\",\n",
    "                      input_filter=\"$[1:]\", #remove first column since the model isn't, and shouldn't be fitted\n",
    "                                            #with the targets \n",
    "                      output_filter=\"$[0,-1]\") #returns first and last column, which are targets and predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.439944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.267351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.052462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.496286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>9.531392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>11.0</td>\n",
       "      <td>9.841301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.345934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>9.0</td>\n",
       "      <td>11.895207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>10.0</td>\n",
       "      <td>9.898403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>12.0</td>\n",
       "      <td>10.720078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target  Prediction\n",
       "0       8.0    8.439944\n",
       "1       9.0    8.267351\n",
       "2       8.0    8.052462\n",
       "3       9.0    8.496286\n",
       "4       8.0    9.531392\n",
       "..      ...         ...\n",
       "622    11.0    9.841301\n",
       "623    10.0   10.345934\n",
       "624     9.0   11.895207\n",
       "625    10.0    9.898403\n",
       "626    12.0   10.720078\n",
       "\n",
       "[627 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(f\"s3://{bucket_name}/predictions/test.csv.out\",names=[\"Target\",\"Prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
